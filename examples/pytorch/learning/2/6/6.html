<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>API Documentation</title>
    <link rel="stylesheet" href="http://121.184.63.113:4000/css/basic.css">
    <link rel="stylesheet" href="http://121.184.63.113:4000/css/default.min.css">
    <script src="http://121.184.63.113:4000/js/highlight.js"></script>
    <!-- <script src="http://121.184.63.113:4000/js/tex-chtml.js"></script> -->
    <script>
      window.MathJax = {
        loader: { load: ['[tex]/ams'] },
        tex: {
          packages: ['base', 'ams'],
        },
        chtml: {
          fontURL: 'http://121.184.63.113:4000/fonts/woff-v2' // CDN 경로 명시
        }
      };
    </script>
    <script type="text/javascript" id="MathJax-script" async
    src="http://121.184.63.113:4000/js/tex-chtml.js">
    </script>
    <script>hljs.initHighlightingOnLoad();</script>
  </head>
  <body class="api-docs">

    

     <!-- explain-1 -->
    <div class="container">
      <h1>딥러닝 구조 및 용어</h1>
      <hr></hr>
      <h2 style="margin-bottom: 3px;">손실함수</h2>
      <p>손실 함수는 머신러닝 모델의 성능을 정량적으로 평가하는 데 사용되는 함수입니다. 모델이 예측한 출력값과 실제 값(정답) 간의 차이를 측정하여, 모델이 얼마나 잘 작동하고 있는지를 나타냅니다. 손실 함수의 값이 작을수록 모델의 예측이 더 정확하다는 것을 의미합니다.</p>
      <br></br>
      <div class="image-div">
        <img src="http://121.184.63.113:4000/images/torch_261.svg" alt="Placeholder image" />
      </div>
      <br></br>
      <h3 style="margin-bottom: 3px;">손실함수 - 1 : 평균 제곱 오차</h3>
      <p>평균 제곱 오차는 예측값과 실제 값 사이의 차이를 측정하기 위해 사용하는 대표적인 손실 함수입니다. MSE는 오차(예측값 - 실제값)의 제곱을 평균화하여, 모델의 예측이 실제 값과 얼마나 가까운지 평가합니다.</p>
    </div>
    

    <div style="font-size: large;">
      \[
      \text{MSE Loss} = \frac{1}{N} \sum_{i=1}^{N} \left( y_i - \hat{y}_i \right)^2
      \]
    </div>

    <!-- code-1 -->
    <div class="code"> 
      <h3>A 를 실행하여 결과를 확인하세요</h3>
      <div class="code_div">
        <pre>
          <code class="language-python">
import numpy as np

# MSE 함수 정의
def mse(y_true, y_pred):
    """
    MSE (Mean Squared Error) 계산 함수
    y_true: 실제값 배열
    y_pred: 예측값 배열
    """
    return np.mean((y_true - y_pred) ** 2)

# 실제값과 예측값
y_true = np.array([1.5, 2.0, 3.5, 4.0])  # 실제값
y_pred = np.array([1.4, 2.2, 3.0, 4.5])  # 예측값

# MSE 계산
mse_value = mse(y_true, y_pred)

# 결과 출력
print("실제값:", y_true)
print("예측값:", y_pred)
print("MSE:", mse_value)
</code>
</pre>
</div>
      <br></br>
    </div >

    <!-- explain-2 -->
    <div class="container">
      <h3 style="margin-bottom: 3px;">손실함수 - 2 : 크로스 엔트로피 오차 - 다중</h3>
      <p>크로스 엔트로피 오차는 분류 문제에서 주로 사용되는 손실 함수로, 모델의 출력 확률 분포와 실제 클래스 분포 간의 차이를 측정합니다. 출력값이 확률 값(예: Softmax 출력)일 때, 실제 클래스에 대한 예측이 얼마나 잘 맞았는지를 평가하는 데 적합합니다.</p>
      <div>
        <p style="font-size: large;">
          \[
          \text{Loss} = -\frac{1}{N} \sum_{i=1}^{N} \sum_{c=1}^{C} y_{i,c} \log(\hat{y}_{i,c})
          \]
        </p>
        <p><strong>설명:</strong></p>
        <p>\( N \): 전체 샘플 수</p>
        <p>\( C \): 클래스의 개수</p>
        <p>\( y_{i,c} \): 샘플 \( i \)와 클래스 \( c \)에 대한 실제 값</p>
        <p>\( \hat{y}_{i,c} \): 샘플 \( i \)와 클래스 \( c \)에 대한 예측 확률</p>
        <br></br>
      </div>
      
    </div>

    <!-- code-2 -->
    <div class="code"> 
      <h3>B 를 실행하여 결과를 확인하세요</h3>
      <div class="code_div">
        <pre>
          <code class="language-python">
import numpy as np

# 예제 데이터
y_true = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])  # 실제 값 (원-핫 인코딩)
y_pred = np.array([[0.7, 0.2, 0.1], [0.1, 0.8, 0.1], [0.2, 0.3, 0.5]])  # 예측 확률 분포

# 크로스 엔트로피 오차 계산
def cross_entropy_loss_multi(y_true, y_pred):
    # 로그를 계산하기 전에 아주 작은 값을 더해 로그 계산 오류 방지 (숫자 안정성)
    epsilon = 1e-15
    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
    # 크로스 엔트로피 공식 적용
    loss = -np.sum(y_true * np.log(y_pred)) / y_true.shape[0]
    return loss

loss = cross_entropy_loss_multi(y_true, y_pred)
print(f"다중 클래스 분류 크로스 엔트로피 손실: {loss:.4f}")
</code>
</pre>
</div>
      <br></br>
    </div >

     <!-- explain-3 -->
    <div class="container">
      <h3 style="margin-bottom: 3px;">손실함수 - 3 : 크로스 엔트로피 오차 - 이진</h3>
      <p>크로스 엔트로피 오차는 분류 문제에서 주로 사용되는 손실 함수로, 모델의 출력 확률 분포와 실제 클래스 분포 간의 차이를 측정합니다. 출력값이 확률 값(예: Softmax 출력)일 때, 실제 클래스에 대한 예측이 얼마나 잘 맞았는지를 평가하는 데 적합합니다.</p>
      <div>
        <p style="font-size: large; overflow: auto;">
          \[
          \text{Loss} = -\frac{1}{N} \sum_{i=1}^{N} \left( y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i) \right)
          \]
        </p>
        <p><strong>설명:</strong></p>
        <p>\( N \): 전체 샘플 수</p>
        <p>\( y_i \): 샘플 \( i \)의 실제 값 (0 또는 1)</p>
        <p>\( \hat{y}_i \): 샘플 \( i \)의 예측 확률 (0에서 1 사이 값)</p>
        <p>\( \log \): 자연 로그 함수</p>
        <br></br>
      </div>      
    </div>

    <!-- code-3 -->
    <div class="code"> 
      <h3>C 를 실행하여 결과를 확인하세요</h3>
      <div class="code_div">
        <pre>
          <code class="language-python">
import numpy as np

# 예제 데이터
y_true = np.array([1, 0, 1])  # 실제 값 (0 또는 1)
y_pred = np.array([0.9, 0.2, 0.8])  # 예측 확률 (0~1 사이)

# 크로스 엔트로피 오차 계산
def cross_entropy_loss_binary(y_true, y_pred):
    # 로그를 계산하기 전에 아주 작은 값을 더해 로그 계산 오류 방지 (숫자 안정성)
    epsilon = 1e-15
    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
    # 크로스 엔트로피 공식 적용
    loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))
    return loss

loss = cross_entropy_loss_binary(y_true, y_pred)
print(f"이진 분류 크로스 엔트로피 손실: {loss:.4f}")
</code>
</pre>
</div>
      <br></br>
    </div >


    <div class="container">
      <p>© JAJUCHA www.jajucha.com</p>
    </div>
  </div >


    
  </body>
</html>
