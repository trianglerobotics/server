<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>API Documentation</title>
    <link rel="stylesheet" href="http://121.184.63.113:4000/css/basic.css">
    <link rel="stylesheet" href="http://121.184.63.113:4000/css/default.min.css">
    <script src="http://121.184.63.113:4000/js/highlight.js"></script>
    <script type="text/javascript" id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
    </script>
    <script>hljs.initHighlightingOnLoad();</script>
  </head>
  <body class="api-docs">

    

     <!-- explain-1 -->
    <div class="container">
      <h1>딥러닝 구조 및 용어</h1>
      <hr></hr>
      <h2 style="margin-bottom: 3px;">활성화 함수</h2>
      <p>활성화 함수는 인공신경망에서 뉴런의 가중합(Weighted Sum)에 비선형성을 추가하는 역할을 하는 함수입니다. 활성화 함수는 뉴런의 최종 출력을 결정하며, 신경망이 복잡한 패턴과 비선형 관계를 학습할 수 있게 합니다.</p>
      <br></br>
      <div class="image-div">
        <img src="http://121.184.63.113:4000/images/torch_251.svg" alt="Placeholder image" />
      </div>
      <br></br>
      <h3 style="margin-bottom: 3px;">활성화 함수 - 1 : 시그모이드 함수</h3>
      <p>시그모이드 함수는 입력값을 0과 1 사이의 값으로 변환하는 비선형 활성화 함수입니다. 이 함수는 입력값의 크기를 제한하고, 신경망의 출력값을 확률처럼 해석할 수 있도록 도와줍니다.</p>
      <div class="image-div">
        <img src="http://121.184.63.113:4000/images/torch_252.svg" alt="Placeholder image" />
      </div>
    </div>
    <!-- code-1 -->
    <div class="code"> 
      <h3>A 를 실행하여 결과를 확인하세요</h3>
      <div class="code_div">
        <pre>
          <code class="language-python">
import numpy as np

# Sigmoid 함수 정의
def sigmoid(x):
    """
    Sigmoid 활성화 함수
    입력값을 0과 1 사이로 변환합니다.
    """
    return 1 / (1 + np.exp(-x))

# 입력 데이터 (예: 여러 값)
inputs = np.array([-3.0, -1.0, 0.0, 1.0, 3.0])

# Sigmoid 함수 적용
outputs = sigmoid(inputs)

# 결과 출력
print("입력값:", inputs)
print("Sigmoid 출력값:", outputs)
</code>
</pre>
</div>
      <br></br>
    </div >

    <!-- explain-2 -->
    <div class="container">
      <h3 style="margin-bottom: 3px;">활성화 함수 - 2 : 렐루 함수</h3>
      <p>ReLU 함수는 신경망에서 가장 널리 사용되는 활성화 함수 중 하나입니다. 간단한 수식과 빠른 계산 속도로 인해 딥러닝 모델에서 기본 활성화 함수로 자주 사용됩니다</p>
      <div class="image-div">
        <img src="http://121.184.63.113:4000/images/torch_253.svg" alt="Placeholder image" />
      </div>
    </div>

    <!-- code-2 -->
    <div class="code"> 
      <h3>B 를 실행하여 결과를 확인하세요</h3>
      <div class="code_div" >
        <pre>
          <code class="language-python">
import numpy as np

# ReLU 함수 정의
def relu(x):
    """
    ReLU 활성화 함수
    입력값이 0보다 작으면 0을 반환하고, 0 이상이면 그대로 반환합니다.
    """
    return np.maximum(0, x)

# 입력 데이터 (예: 네 개의 값)
inputs = np.array([-3.0, -1.0, 0.0, 1.0, 3.0])

# ReLU 함수 적용
outputs = relu(inputs)

# 결과 출력
print("입력값:", inputs)
print("ReLU 출력값:", outputs)
</code>
</pre>
</div>
      <br></br>
    </div >

    <!-- explain-3 -->
    <div class="container">
      <h3 style="margin-bottom: 3px;">활성화 함수 - 3 :소프트맥스 함수 </h3>
      <p>소프트맥스(Softmax) 함수는 다중 클래스 분류 문제에서 사용되는 활성화 함수로, 각 클래스에 속할 확률을 추정합니다. 이 함수는 입력받은 값을 0과 1 사이의 값으로 정규화하며, 모든 출력값의 합은 1이 됩니다. 이러한 특성 때문에 소프트맥스 함수는 각 클래스에 대한 확률 분포를 나타내는 데 적합합니다</p>
      <div>
        <p>
          \[
          \text{Softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{C} e^{z_j}}
          \]
        </p>
        <p><strong>설명:</strong></p>
        <p>\( z_i \): 샘플 \( i \)에 대한 입력값 (로짓)</p>
        <p>\( C \): 전체 클래스 수</p>
        <p>\( e \): 자연 상수 (오일러 수, \( e \approx 2.718 \))</p>
        <p>\( \text{Softmax}(z_i) \): 클래스 \( i \)에 속할 확률</p>
        <p>\( \sum_{j=1}^{C} e^{z_j} \): 모든 클래스의 지수 값 합계</p>
        <br></br>
      </div>
      
    </div>

    <!-- code-3 -->
    <div class="code"> 
      <h3>C 를 실행하여 결과를 확인하세요</h3>
      <div class="code_div">
      <pre>
        <code class="language-python">
import numpy as np

# Softmax 함수 정의
def softmax(x):
    """
    Softmax 활성화 함수
    입력값 배열을 확률 분포로 변환합니다.
    """
    # 입력값의 지수 계산
    exp_x = np.exp(x - np.max(x))  # 오버플로 방지를 위해 입력에서 최대값을 뺌
    return exp_x / np.sum(exp_x)

# 입력 데이터 (예: 네 클래스의 로짓 값)
inputs = np.array([2.0, 1.0, 0.1])

# Softmax 함수 적용
outputs = softmax(inputs)

# 결과 출력
print("입력값:", inputs)
print("Softmax 출력값 (확률 분포):", outputs)
print("출력값의 합:", np.sum(outputs))  # 확률의 총합은 항상 1
</code>
</pre>
</div>
      <br></br>
    </div >
    <div class="container">
      <p>© JAJUCHA www.jajucha.com</p>
    </div>
  </div >


    
  </body>
</html>
